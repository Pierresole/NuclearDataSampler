{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2823e1e",
   "metadata": {},
   "source": [
    "# Step 1: Check Nominal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd47bef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ENDFtk.tree import Tape\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tape = Tape.from_file('/home/sole-pie01/ndlib/tendl23-neutron/n-Fe056.tendl.txt')\n",
    "mf42 = tape.MAT(tape.material_numbers[0]).MF(4).MT(2).parse()\n",
    "mf342 = tape.MAT(tape.material_numbers[0]).MF(34).MT(2).parse()\n",
    "sampled_params = []\n",
    "\n",
    "mf42.distributions.legendre.angular_distributions[0].legendre_order\n",
    "# mf342.reactions[0].legendre_blocks[0].data[0].energies[-1]\n",
    "\n",
    "# mf42.distributions.legendre.incident_energies[-1]\n",
    "# for index, distrib in enumerate(mf42.distributions.legendre.angular_distributions):\n",
    "#     print(f\"Number of orders={distrib.legendre_order} at energy={distrib.E}\")\n",
    "#     print(distrib.coefficients[:])\n",
    "#     for j, spingparity in enumerate(orbital.j_values):\n",
    "#         print(f\"Orbital, l={l}, J={j}, pi={spingparity.spin}\")\n",
    "#         print(f\"  {len(spingparity.energies[:])} Energies: {spingparity.energies[:]}\")\n",
    "#         print(f\"  Spacings: {spingparity.average_level_spacings[:]}\")\n",
    "#         sampled_params.append(spingparity.average_level_spacings[:])\n",
    "#         # print(f\"  Gamma widths: {spingparity.average_gamma_widths[:]}\")\n",
    "#         print(f\"  Neutron widths: {spingparity.average_neutron_widths[:]}\")\n",
    "#         sampled_params.append(spingparity.average_neutron_widths[:])\n",
    "        \n",
    "# len(sampled_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56940ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mesh_union(mesh1, mesh2, eps=1e-8):\n",
    "    union = np.unique(np.concatenate((mesh1, mesh2)))\n",
    "    diff = np.diff(union)\n",
    "    mask = diff < eps\n",
    "    if np.any(mask):\n",
    "        keep = np.ones_like(union, dtype=bool)\n",
    "        keep[1:][mask] = False\n",
    "        union = union[keep]\n",
    "    return union\n",
    "\n",
    "def expand_matrix_fast(original_matrix, original_row_mesh, original_col_mesh, union_row_mesh, union_col_mesh):\n",
    "    original_row_mesh = np.array(sorted(original_row_mesh))\n",
    "    original_col_mesh = np.array(sorted(original_col_mesh))\n",
    "    union_row_mesh = np.array(sorted(union_row_mesh))\n",
    "    union_col_mesh = np.array(sorted(union_col_mesh))\n",
    "\n",
    "    original_row_size = len(original_row_mesh) - 1\n",
    "    original_col_size = len(original_col_mesh) - 1\n",
    "\n",
    "    row_indices = np.searchsorted(original_row_mesh, union_row_mesh[:-1], side='right') - 1\n",
    "    col_indices = np.searchsorted(original_col_mesh, union_col_mesh[:-1], side='right') - 1\n",
    "\n",
    "    row_indices = np.clip(row_indices, 0, original_row_size-1)\n",
    "    col_indices = np.clip(col_indices, 0, original_col_size-1)\n",
    "\n",
    "    expanded_matrix = original_matrix[np.ix_(row_indices, col_indices)]\n",
    "    return expanded_matrix\n",
    "\n",
    "def add_matrices_with_mesh(matrixA, rowMeshA, colMeshA, matrixB, rowMeshB, colMeshB, epsilon=1e-8):\n",
    "    if matrixA.size == 0:\n",
    "        return matrixB.copy(), sorted(rowMeshB), sorted(colMeshB)\n",
    "    if matrixB.size == 0:\n",
    "        return matrixA.copy(), sorted(rowMeshA), sorted(colMeshA)\n",
    "\n",
    "    rowMeshA = np.array(sorted(rowMeshA))\n",
    "    colMeshA = np.array(sorted(colMeshA))\n",
    "    rowMeshB = np.array(sorted(rowMeshB))\n",
    "    colMeshB = np.array(sorted(colMeshB))\n",
    "\n",
    "    union_row_mesh = mesh_union(rowMeshA, rowMeshB, epsilon)\n",
    "    union_col_mesh = mesh_union(colMeshA, colMeshB, epsilon)\n",
    "\n",
    "    expandedA = expand_matrix_fast(matrixA, rowMeshA, colMeshA, union_row_mesh, union_col_mesh)\n",
    "    expandedB = expand_matrix_fast(matrixB, rowMeshB, colMeshB, union_row_mesh, union_col_mesh)\n",
    "\n",
    "    result = expandedA + expandedB\n",
    "    return result, union_row_mesh.tolist(), union_col_mesh.tolist()\n",
    "\n",
    "def subblock_to_matrix(subblock):\n",
    "    # LB==5: symmetric, upper triangle stored\n",
    "    if hasattr(subblock, \"LB\") and subblock.LB == 5:\n",
    "        N = subblock.NE - 1\n",
    "        mesh = subblock.energies.to_list()\n",
    "        mat = np.zeros((N, N))\n",
    "        triu_indices = np.triu_indices(N)\n",
    "        mat[triu_indices] = subblock.values.to_list()\n",
    "        mat = mat + mat.T - np.diag(np.diag(mat))\n",
    "        return mat, mesh, mesh\n",
    "    # LB==1: diagonal\n",
    "    elif hasattr(subblock, \"LB\") and subblock.LB == 1:\n",
    "        mesh = subblock.first_array_energies.to_list()\n",
    "        vals = subblock.first_array_fvalues.to_list()\n",
    "        mat = np.diag(vals)\n",
    "        return mat, mesh, mesh\n",
    "    # CovariancePairs (LB==1)\n",
    "    elif hasattr(subblock, \"number_pairs\"):\n",
    "        mesh = subblock.first_array_energies.to_list()\n",
    "        vals = subblock.first_array_fvalues.to_list()\n",
    "        mat = np.diag(vals)\n",
    "        return mat, mesh, mesh\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown subblock type\")\n",
    "\n",
    "def block_to_matrix(block):\n",
    "    # block is ENDFtk.SquareMatrix or ENDFtk.LegendreBlock\n",
    "    # block.data.to_list() gives subblocks\n",
    "    subblocks = block.data.to_list() if hasattr(block, \"data\") else [block]\n",
    "    matrix = np.zeros((0,0))\n",
    "    row_mesh = []\n",
    "    col_mesh = []\n",
    "    for sub in subblocks:\n",
    "        submat, subrow, subcol = subblock_to_matrix(sub)\n",
    "        if matrix.size == 0:\n",
    "            matrix = submat\n",
    "            row_mesh = subrow\n",
    "            col_mesh = subcol\n",
    "        else:\n",
    "            matrix, row_mesh, col_mesh = add_matrices_with_mesh(\n",
    "                matrix, row_mesh, col_mesh, submat, subrow, subcol\n",
    "            )\n",
    "    return matrix, row_mesh, col_mesh\n",
    "\n",
    "def covariance_to_correlation_and_relstd(rel_cov):\n",
    "    diag = np.diag(rel_cov)\n",
    "    std = np.sqrt(np.maximum(diag, 0))\n",
    "    # Avoid division by zero\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        corr = rel_cov / np.outer(std, std)\n",
    "        corr[~np.isfinite(corr)] = 0.0\n",
    "        relstd = std.copy()\n",
    "    return corr, relstd\n",
    "\n",
    "def retrieve_full_covariance_matrix(mt2):\n",
    "    NL = mt2.NL\n",
    "    NL1 = mt2.NL1\n",
    "    nblocks = mt2.number_legendre_blocks\n",
    "    blocks = mt2.legendre_blocks.to_list()\n",
    "    # First, collect all unique energy mesh points for all blocks\n",
    "    all_mesh = set()\n",
    "    for block in blocks:\n",
    "        for sub in block.data.to_list():\n",
    "            if hasattr(sub, \"LB\") and sub.LB == 5:\n",
    "                all_mesh.update(sub.energies.to_list())\n",
    "            elif hasattr(sub, \"LB\") and sub.LB == 1:\n",
    "                all_mesh.update(sub.first_array_energies.to_list())\n",
    "            elif hasattr(sub, \"number_pairs\"):\n",
    "                all_mesh.update(sub.first_array_energies.to_list())\n",
    "    all_mesh = sorted(all_mesh)\n",
    "    N = len(all_mesh) - 1\n",
    "\n",
    "    # Prepare the full relative covariance matrix\n",
    "    full_rel_cov = np.zeros((NL*N, NL1*N))\n",
    "    # For each block (l, l1), fill the corresponding submatrix\n",
    "    for idx, block in enumerate(blocks):\n",
    "        l = block.L\n",
    "        l1 = block.L1\n",
    "        mat, row_mesh, col_mesh = block_to_matrix(block)\n",
    "        # Expand to the global mesh\n",
    "        mat_expanded = expand_matrix_fast(mat, row_mesh, col_mesh, all_mesh, all_mesh)\n",
    "        # Place in the full matrix\n",
    "        full_rel_cov[(l-1)*N:l*N, (l1-1)*N:l1*N] = mat_expanded\n",
    "        if l != l1:\n",
    "            # Fill symmetric block\n",
    "            full_rel_cov[(l1-1)*N:l1*N, (l-1)*N:l*N] = mat_expanded.T\n",
    "\n",
    "    return full_rel_cov\n",
    "mt2 = mf342.reactions.to_list()[0]\n",
    "relativ_cov = retrieve_full_covariance_matrix(mt2)\n",
    "# Count non zero in the diagonal\n",
    "np.count_nonzero(np.diag(relativ_cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93040ed3",
   "metadata": {},
   "source": [
    "## Step 2: Build sample matrix from perturbed tapes and verify relative covariance\n",
    "We read sampled_tape_random1.endf ... sampled_tape_random9.endf, interpolate MF(4) Legendre coefficients at midpoints of the global covariance energy mesh (from MF(34)), flatten order-by-order to form a parameter vector per sample, then compare the empirical relative covariance to the MF(34) reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cbb6c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 sampled tapes: ['sampled_tape_random1.endf', 'sampled_tape_random2.endf', 'sampled_tape_random3.endf', 'sampled_tape_random4.endf', 'sampled_tape_random5.endf', 'sampled_tape_random6.endf', 'sampled_tape_random7.endf', 'sampled_tape_random8.endf', 'sampled_tape_random9.endf', 'sampled_tape_random10.endf']\n",
      "Covariance global mesh: 5 points -> 4 bins\n",
      "Orders present (from covariance blocks): [1, 2, 3, 4, 5, 6]\n",
      "Scanning sample tapes for available orders...\n",
      "  sampled_tape_random1.endf: min_common_order=1\n",
      "  sampled_tape_random2.endf: min_common_order=1\n",
      "  sampled_tape_random3.endf: min_common_order=1\n",
      "  sampled_tape_random4.endf: min_common_order=1\n",
      "  sampled_tape_random5.endf: min_common_order=1\n",
      "  sampled_tape_random6.endf: min_common_order=1\n",
      "  sampled_tape_random7.endf: min_common_order=1\n",
      "  sampled_tape_random8.endf: min_common_order=1\n",
      "  sampled_tape_random9.endf: min_common_order=1\n",
      "  sampled_tape_random10.endf: min_common_order=1\n",
      "Selected active orders: [1]\n",
      "Sample matrix shape: (10, 4) (using 10 tapes)\n",
      "Diagonal rel error stats: mean=4.114e-01, median=-3.454e-01, max_abs=3.042e+00\n",
      "Correlation relative Frobenius diff: 7.908e-01\n",
      "Order l=1: diag RMS rel error=1.599e+00\n",
      "Verification results prepared.\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "from pathlib import Path\n",
    "from ENDFtk.tree import Tape\n",
    "import numpy as np\n",
    "\n",
    "sampled_dir = Path('../FreeGazScattering')\n",
    "pattern = re.compile(r'^sampled_tape_random(\\d+)\\.endf$')\n",
    "\n",
    "sample_files = []\n",
    "if sampled_dir.is_dir():\n",
    "    for fname in os.listdir(sampled_dir):\n",
    "        m = pattern.match(fname)\n",
    "        if m:\n",
    "            sample_files.append((int(m.group(1)), sampled_dir / fname))\n",
    "\n",
    "sample_files.sort()\n",
    "print(f\"Found {len(sample_files)} sampled tapes: {[p.name for _, p in sample_files]}\")\n",
    "if not sample_files:\n",
    "    raise FileNotFoundError(\"No sampled_tape_random*.endf files found in ../FreeGazScattering\")\n",
    "\n",
    "blocks = mf342.reactions[0].legendre_blocks.to_list()\n",
    "all_mesh = set()\n",
    "for block in blocks:\n",
    "    for sub in block.data.to_list():\n",
    "        if hasattr(sub, 'LB') and sub.LB == 5:\n",
    "            all_mesh.update(sub.energies.to_list())\n",
    "        elif hasattr(sub, 'LB') and sub.LB == 1:\n",
    "            all_mesh.update(sub.first_array_energies.to_list())\n",
    "        elif hasattr(sub, 'number_pairs'):\n",
    "            all_mesh.update(sub.first_array_energies.to_list())\n",
    "all_mesh = sorted(all_mesh)\n",
    "energy_bins = list(zip(all_mesh[:-1], all_mesh[1:]))\n",
    "midpoints = np.array([0.5*(a+b) for a,b in energy_bins])\n",
    "print(f\"Covariance global mesh: {len(all_mesh)} points -> {len(energy_bins)} bins\")\n",
    "\n",
    "orders_present_cov = sorted({blk.L for blk in blocks})\n",
    "print(\"Orders present (from covariance blocks):\", orders_present_cov)\n",
    "\n",
    "from bisect import bisect_right\n",
    "\n",
    "def extract_min_common_order(tape_obj, mt=2):\n",
    "    try:\n",
    "        mf4_local = tape_obj.MAT(tape_obj.material_numbers[0]).MF(4).MT(mt).parse()\n",
    "    except Exception as e:\n",
    "        print(f\"  [WARN] Failed to parse MF4/MT{mt}: {e}\")\n",
    "        return -1\n",
    "    leg = mf4_local.distributions.legendre\n",
    "    distributions = getattr(leg, 'angular_distributions', [])\n",
    "    if not distributions:\n",
    "        return -1\n",
    "    # mf42.distributions.legendre.angular_distributions[0].legendre_order\n",
    "    per_energy_actual = [dist.legendre_order + 1 for dist in distributions]\n",
    "    # print(per_energy_actual)\n",
    "    return min(per_energy_actual) if per_energy_actual else -1\n",
    "\n",
    "def interpolate_legendre_coeffs(tape_obj, target_orders, mt=2, energies=midpoints):\n",
    "    mf4_local = tape_obj.MAT(tape_obj.material_numbers[0]).MF(4).MT(mt).parse()\n",
    "    leg = mf4_local.distributions.legendre\n",
    "    E_grid = np.array(leg.incident_energies[:])\n",
    "    distributions = leg.angular_distributions\n",
    "    max_order_needed = max(target_orders)\n",
    "    coeff_grid = np.full((len(E_grid), max_order_needed+1), np.nan)\n",
    "    for i, dist in enumerate(distributions):\n",
    "        coeffs = np.array(dist.coefficients[:])\n",
    "        take = min(len(coeffs)-1, max_order_needed)\n",
    "        if take >= 0:\n",
    "            coeff_grid[i, :take+1] = coeffs[:take+1]\n",
    "    result_rows = []\n",
    "    for l in target_orders:\n",
    "        y = coeff_grid[:, l]\n",
    "        mask = ~np.isnan(y)\n",
    "        if mask.sum() < 2:\n",
    "            interp_vals = np.zeros_like(energies)\n",
    "        else:\n",
    "            E_valid = E_grid[mask]\n",
    "            y_valid = y[mask]\n",
    "            interp_vals = np.interp(energies, E_valid, y_valid)\n",
    "        result_rows.append(interp_vals)\n",
    "    arr = np.vstack(result_rows)\n",
    "    return arr.reshape(len(target_orders)*len(energies))\n",
    "\n",
    "print(\"Scanning sample tapes for available orders...\")\n",
    "nominal_min_order = extract_min_common_order(tape)\n",
    "per_sample_info = []  # (idx, path, min_order)\n",
    "for idx, p in sample_files:\n",
    "    t = Tape.from_file(str(p))\n",
    "    mo = extract_min_common_order(t)\n",
    "    per_sample_info.append((idx, p, mo))\n",
    "    print(f\"  {p.name}: min_common_order={mo}\")\n",
    "\n",
    "valid_samples = [(i,p,mo) for i,p,mo in per_sample_info if mo >= 1]\n",
    "if not valid_samples:\n",
    "    print(\"[ERROR] None of the sampled tapes provide Legendre coefficients (order >=1). Aborting verification.\")\n",
    "    verification_results = dict(\n",
    "        sample_files=[str(p) for _,p in sample_files],\n",
    "        active_orders=[],\n",
    "        note=\"No valid sampled tapes with MF(4) Legendre distributions.\"\n",
    "    )\n",
    "else:\n",
    "    min_common_order = min([nominal_min_order] + [mo for _,_,mo in valid_samples])\n",
    "    active_orders = [l for l in orders_present_cov if l <= min_common_order]\n",
    "    print(\"Selected active orders:\", active_orders)\n",
    "    if not active_orders:\n",
    "        print(\"[ERROR] No overlapping active orders between nominal and samples.\")\n",
    "        verification_results = dict(\n",
    "            sample_files=[str(p) for _,p,_ in valid_samples],\n",
    "            active_orders=[],\n",
    "            note=\"No overlapping orders.\"\n",
    "        )\n",
    "    else:\n",
    "        nominal_vec = interpolate_legendre_coeffs(tape, active_orders)\n",
    "        vectors = []\n",
    "        used_files = []\n",
    "        for idx, path, mo in valid_samples:\n",
    "            t = Tape.from_file(str(path))\n",
    "            vec = interpolate_legendre_coeffs(t, active_orders)\n",
    "            vectors.append(vec)\n",
    "            used_files.append(path)\n",
    "        sample_matrix = np.vstack(vectors)\n",
    "        print(f\"Sample matrix shape: {sample_matrix.shape} (using {len(used_files)} tapes)\")\n",
    "        abs_tol = 1e-14\n",
    "        nominal_safe = nominal_vec.copy()\n",
    "        small_mask = np.abs(nominal_safe) < abs_tol\n",
    "        nominal_safe[small_mask] = 1.0\n",
    "        rel_deltas = (sample_matrix - nominal_vec) / nominal_safe\n",
    "        rel_deltas[:, small_mask] = 0.0\n",
    "        empirical_rel_cov = np.cov(rel_deltas, rowvar=False, bias=False)\n",
    "        N_bins = len(energy_bins)\n",
    "        ref_indices = []\n",
    "        for l in active_orders:\n",
    "            ref_indices.extend(range((l-1)*N_bins, (l-1)*N_bins + N_bins))\n",
    "        ref_indices = np.array(ref_indices)\n",
    "        reference_rel_cov = relativ_cov[np.ix_(ref_indices, ref_indices)]\n",
    "        ref_diag = np.diag(reference_rel_cov)\n",
    "        emp_diag = np.diag(empirical_rel_cov)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            diag_rel_error = np.where(ref_diag != 0, (emp_diag - ref_diag)/ref_diag, 0.0)\n",
    "        print(f\"Diagonal rel error stats: mean={np.mean(diag_rel_error):.3e}, median={np.median(diag_rel_error):.3e}, max_abs={np.max(np.abs(diag_rel_error)):.3e}\")\n",
    "        ref_std = np.sqrt(np.maximum(ref_diag, 0))\n",
    "        emp_std = np.sqrt(np.maximum(emp_diag, 0))\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            ref_corr = reference_rel_cov / np.outer(ref_std, ref_std)\n",
    "            emp_corr = empirical_rel_cov / np.outer(emp_std, emp_std)\n",
    "            ref_corr[~np.isfinite(ref_corr)] = 0.0\n",
    "            emp_corr[~np.isfinite(emp_corr)] = 0.0\n",
    "        corr_rel_diff = np.linalg.norm(emp_corr - ref_corr) / (np.linalg.norm(ref_corr) + 1e-30)\n",
    "        print(f\"Correlation relative Frobenius diff: {corr_rel_diff:.3e}\")\n",
    "        for i, l in enumerate(active_orders):\n",
    "            seg = slice(i*N_bins, (i+1)*N_bins)\n",
    "            ref_d = ref_diag[seg]\n",
    "            emp_d = emp_diag[seg]\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                rel_err = np.where(ref_d != 0, (emp_d - ref_d)/ref_d, 0.0)\n",
    "            rms = np.sqrt(np.mean(rel_err**2)) if rel_err.size else 0.0\n",
    "            print(f\"Order l={l}: diag RMS rel error={rms:.3e}\")\n",
    "        verification_results = dict(\n",
    "            sample_files=[str(p) for p in used_files],\n",
    "            dropped_files=[str(p) for _,p,mo in per_sample_info if p not in used_files],\n",
    "            active_orders=active_orders,\n",
    "            energy_midpoints=midpoints,\n",
    "            sample_matrix=sample_matrix.tolist(),\n",
    "            relative_deltas=rel_deltas.tolist(),\n",
    "            empirical_rel_cov=empirical_rel_cov.tolist(),\n",
    "            reference_rel_cov=reference_rel_cov.tolist(),\n",
    "            diag_rel_error=diag_rel_error.tolist(),\n",
    "            corr_rel_frobenius_diff=float(corr_rel_diff),\n",
    "        )\n",
    "print(\"Verification results prepared.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
