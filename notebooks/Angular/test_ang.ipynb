{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc64c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ENDFtk\n",
    "import ndsampler\n",
    "from ndsampler.angular.Uncertainty_Angular import Uncertainty_Angular\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0cfa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tape = ENDFtk.tree.Tape.from_file('/home/pierre/Projects/LIB/JEFF33/26-Fe-56g.jeff33')\n",
    "mf4mt2 = tape.MAT(tape.material_numbers[0]).MF(4).MT(2).parse()\n",
    "angulard = tape.materials.front().section(4, 2).parse()\n",
    "angularu = tape.materials.front().section(34, 2).parse()\n",
    "uncertainty = Uncertainty_Angular(mf4mt2, angularu, mt_number=2)\n",
    "reduced_size = len(uncertainty.covariance_index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a33639",
   "metadata": {},
   "source": [
    "### Check eigenvalues of reduced matrix\n",
    "\n",
    "**Better approach**: Eigenvalue thresholding with rescaling to preserve trace:\n",
    "1. Decompose: $\\Sigma = V \\Lambda V^T$\n",
    "2. Threshold: $\\tilde{\\Lambda} = \\max(\\Lambda, \\epsilon)$ where $\\epsilon$ is small positive value\n",
    "3. **Rescale** to preserve trace: $\\tilde{\\Lambda}' = \\tilde{\\Lambda} \\cdot \\frac{\\text{tr}(\\Lambda)}{\\text{tr}(\\tilde{\\Lambda})}$\n",
    "4. Reconstruct: $\\tilde{\\Sigma} = V \\tilde{\\Lambda}' V^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df73eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_cov = uncertainty.relative_covariance_matrix_full\n",
    "eigenvals = np.linalg.eigvals(reduced_cov)\n",
    "min_eigenval = np.min(eigenvals)\n",
    "max_eigenval = np.max(eigenvals)\n",
    "\n",
    "print(f\"✓ Reduced covariance matrix eigenvalues:\")\n",
    "print(f\"    Min eigenvalue: {min_eigenval:.6e}\")\n",
    "print(f\"    Max eigenvalue: {max_eigenval:.6e}\")\n",
    "print(f\"    Condition number: {max_eigenval/max_eigenval:.2e}\")\n",
    "\n",
    "# Test Cholesky decomposition\n",
    "try:\n",
    "    L_test = np.linalg.cholesky(reduced_cov)\n",
    "    print(f\"✓ Matrix is positive definite (Cholesky successful)\")\n",
    "    \n",
    "    # Verify reconstruction\n",
    "    reconstructed = L_test @ L_test.T\n",
    "    frobenius_error = np.linalg.norm(reconstructed - reduced_cov, 'fro')\n",
    "    print(f\"✓ Cholesky reconstruction error: {frobenius_error:.2e}\")\n",
    "    \n",
    "except np.linalg.LinAlgError:\n",
    "    print(f\"✗ Matrix is NOT positive definite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bfbe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "np.random.seed(42)  # Set random seed for reproducibility\n",
    "samples = uncertainty.sample_parameters(num_samples=n_samples, sampling_method=\"Simple\", mode=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3c4957",
   "metadata": {},
   "source": [
    "# $\\chi^2$ and K-S tests of Mahalanobis distance of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_inv = np.linalg.inv(reduced_cov)\n",
    "\n",
    "# Center samples (should be approximately zero mean)\n",
    "mu_empirical = np.mean(samples, axis=0)\n",
    "mu_0 = np.zeros_like(mu_empirical)  # Expected mean is zero\n",
    "\n",
    "print(f\"Sample statistics:\")\n",
    "print(f\"  Mean norm: {np.linalg.norm(mu_empirical):.6e} (should be ≈0)\")\n",
    "print(f\"  Max |mean|: {np.max(np.abs(mu_empirical)):.6e}\")\n",
    "\n",
    "# Compute Mahalanobis distances\n",
    "D_squared = []\n",
    "for x_i in samples:\n",
    "    diff = x_i - mu_0\n",
    "    D2 = diff @ Sigma_inv @ diff\n",
    "    D_squared.append(D2)\n",
    "\n",
    "D_squared = np.array(D_squared)\n",
    "p = len(uncertainty.covariance_index_map)  # Number of parameters\n",
    "\n",
    "# Statistical analysis\n",
    "print(f\"\\nMahalanobis distance statistics:\")\n",
    "print(f\"  Mean D²:    {np.mean(D_squared):.4f}  (expected: {p:.1f})\")\n",
    "print(f\"  Median D²:  {np.median(D_squared):.4f}\")\n",
    "print(f\"  Std D²:     {np.std(D_squared):.4f}  (expected: {np.sqrt(2*p):.4f})\")\n",
    "print(f\"  Bias:       {np.mean(D_squared) - p:.4f}  ({(np.mean(D_squared)/p - 1)*100:+.2f}%)\")\n",
    "\n",
    "# Statistical tests\n",
    "t_stat, t_p = stats.ttest_1samp(D_squared, p)\n",
    "chi2_dist = stats.chi2(df=p)\n",
    "ks_stat, ks_p = stats.kstest(D_squared, lambda x: chi2_dist.cdf(x))\n",
    "\n",
    "print(f\"\\nStatistical Tests:\")\n",
    "print(f\"  t-test (mean = {p}):  t={t_stat:.4f}, p={t_p:.4f}\")\n",
    "print(f\"  Result: {'✓ PASS' if t_p > 0.05 else '✗ FAIL'} (α=0.05)\")\n",
    "print(f\"  K-S test:             D={ks_stat:.4f}, p={ks_p:.4f}\")\n",
    "print(f\"  Result: {'✓ PASS' if ks_p > 0.05 else '✗ FAIL'} (α=0.05)\")\n",
    "\n",
    "# Final validation\n",
    "bias_percent = abs((np.mean(D_squared)/p - 1)*100)\n",
    "print(f\"\\n=== VALIDATION SUMMARY ===\")\n",
    "if bias_percent < 2 and t_p > 0.05 and ks_p > 0.05:\n",
    "    print(f\"✅ SUCCESS: NDSampler implementation is CORRECT!\")\n",
    "    print(f\"   Bias: {(np.mean(D_squared)/p - 1)*100:+.2f}% (< 2%)\")\n",
    "    print(f\"   Statistical tests: PASSED\")\n",
    "else:\n",
    "    print(f\"❌ ISSUES DETECTED:\")\n",
    "    if bias_percent >= 2:\n",
    "        print(f\"   Large bias: {(np.mean(D_squared)/p - 1)*100:+.2f}%\")\n",
    "    if t_p <= 0.05:\n",
    "        print(f\"   t-test failed (p={t_p:.4f})\")\n",
    "    if ks_p <= 0.05:\n",
    "        print(f\"   K-S test failed (p={ks_p:.4f})\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Histogram\n",
    "ax = axes[0]\n",
    "ax.hist(D_squared, bins=30, density=True, alpha=0.7, color='skyblue', label='Empirical')\n",
    "\n",
    "# Theoretical chi-squared\n",
    "x_theory = np.linspace(0, np.max(D_squared), 1000)\n",
    "y_theory = chi2_dist.pdf(x_theory)\n",
    "ax.plot(x_theory, y_theory, 'r-', linewidth=2, label=f'χ²({p}) theory')\n",
    "\n",
    "ax.set_xlabel('Mahalanobis Distance²')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Mahalanobis Distance Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot\n",
    "ax = axes[1]\n",
    "stats.probplot(D_squared, dist=stats.chi2(df=p), plot=ax)\n",
    "ax.set_title('Q-Q Plot vs χ² Distribution')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDistribution analysis:\")\n",
    "print(f\"  Sample min/max: [{np.min(D_squared):.2f}, {np.max(D_squared):.2f}]\")\n",
    "print(f\"  Theoretical mean/std: {p:.1f} / {np.sqrt(2*p):.2f}\")\n",
    "print(f\"  Empirical mean/std: {np.mean(D_squared):.2f} / {np.std(D_squared):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97678183",
   "metadata": {},
   "source": [
    "## Marginals tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ea470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive marginal normality tests for all 168 parameters\n",
    "print(f\"Analyzing {samples.shape[1]} parameters from {samples.shape[0]} samples\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test each parameter for normality\n",
    "normality_results = []\n",
    "failed_params = []\n",
    "\n",
    "for i in range(samples.shape[1]):\n",
    "    param_samples = samples[:, i]\n",
    "    \n",
    "    # Shapiro-Wilk test (good for n < 5000)\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(param_samples)\n",
    "    \n",
    "    # Kolmogorov-Smirnov test\n",
    "    param_mean = np.mean(param_samples)\n",
    "    param_std = np.std(param_samples, ddof=1)\n",
    "    ks_stat, ks_p = stats.kstest(param_samples, lambda x: stats.norm.cdf(x, loc=param_mean, scale=param_std))\n",
    "    \n",
    "    # Anderson-Darling test\n",
    "    anderson_result = stats.anderson(param_samples, dist='norm')\n",
    "    anderson_stat = anderson_result.statistic\n",
    "    # Critical value at 5% significance\n",
    "    anderson_critical = anderson_result.critical_values[2]  # Index 2 corresponds to 5%\n",
    "    anderson_pass = anderson_stat < anderson_critical\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'param_idx': i,\n",
    "        'mean': param_mean,\n",
    "        'std': param_std,\n",
    "        'shapiro_stat': shapiro_stat,\n",
    "        'shapiro_p': shapiro_p,\n",
    "        'ks_stat': ks_stat,\n",
    "        'ks_p': ks_p,\n",
    "        'anderson_stat': anderson_stat,\n",
    "        'anderson_critical': anderson_critical,\n",
    "        'shapiro_pass': shapiro_p > 0.05,\n",
    "        'ks_pass': ks_p > 0.05,\n",
    "        'anderson_pass': anderson_pass\n",
    "    }\n",
    "    normality_results.append(result)\n",
    "    \n",
    "    # Track failures (all three tests must pass)\n",
    "    if not (result['shapiro_pass'] and result['ks_pass'] and result['anderson_pass']):\n",
    "        failed_params.append(i)\n",
    "\n",
    "# Summary statistics\n",
    "n_params = len(normality_results)\n",
    "n_shapiro_pass = sum(r['shapiro_pass'] for r in normality_results)\n",
    "n_ks_pass = sum(r['ks_pass'] for r in normality_results)\n",
    "n_anderson_pass = sum(r['anderson_pass'] for r in normality_results)\n",
    "n_all_pass = sum(r['shapiro_pass'] and r['ks_pass'] and r['anderson_pass'] for r in normality_results)\n",
    "\n",
    "print(f\"\\n=== NORMALITY TEST SUMMARY ===\")\n",
    "print(f\"Total parameters tested: {n_params}\")\n",
    "print(f\"Shapiro-Wilk passed:     {n_shapiro_pass}/{n_params} ({100*n_shapiro_pass/n_params:.1f}%)\")\n",
    "print(f\"K-S test passed:         {n_ks_pass}/{n_params} ({100*n_ks_pass/n_params:.1f}%)\")\n",
    "print(f\"Anderson-Darling passed: {n_anderson_pass}/{n_params} ({100*n_anderson_pass/n_params:.1f}%)\")\n",
    "print(f\"All tests passed:        {n_all_pass}/{n_params} ({100*n_all_pass/n_params:.1f}%)\")\n",
    "\n",
    "if len(failed_params) > 0:\n",
    "    print(f\"\\n⚠️  {len(failed_params)} parameters failed at least one test\")\n",
    "    print(f\"Failed parameter indices: {failed_params[:10]}{'...' if len(failed_params) > 10 else ''}\")\n",
    "else:\n",
    "    print(f\"\\n✅ All {n_params} parameters passed all normality tests!\")\n",
    "\n",
    "# Distribution of p-values (should be uniform under null hypothesis)\n",
    "shapiro_pvals = [r['shapiro_p'] for r in normality_results]\n",
    "ks_pvals = [r['ks_p'] for r in normality_results]\n",
    "\n",
    "# Visualizations\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Histogram of Shapiro-Wilk p-values\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.hist(shapiro_pvals, bins=20, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.axhline(1.0, color='red', linestyle='--', linewidth=2, label='Uniform (expected)')\n",
    "ax1.set_xlabel('Shapiro-Wilk p-value')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Distribution of Shapiro-Wilk p-values')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Histogram of K-S p-values\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.hist(ks_pvals, bins=20, density=True, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "ax2.axhline(1.0, color='red', linestyle='--', linewidth=2, label='Uniform (expected)')\n",
    "ax2.set_xlabel('K-S p-value')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Distribution of K-S p-values')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Scatter: Shapiro vs K-S p-values\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "colors = ['red' if i in failed_params else 'blue' for i in range(n_params)]\n",
    "ax3.scatter(shapiro_pvals, ks_pvals, c=colors, alpha=0.5, s=20)\n",
    "ax3.axhline(0.05, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.axvline(0.05, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('Shapiro-Wilk p-value')\n",
    "ax3.set_ylabel('K-S p-value')\n",
    "ax3.set_title('Correlation: Shapiro vs K-S tests')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Sample Q-Q plots for first 6 parameters\n",
    "for idx, param_idx in enumerate(range(6)):\n",
    "    ax = fig.add_subplot(gs[1 + idx//3, idx%3])\n",
    "    param_samples = samples[:, param_idx]\n",
    "    stats.probplot(param_samples, dist=\"norm\", plot=ax)\n",
    "    \n",
    "    result = normality_results[param_idx]\n",
    "    status = '✓' if result['shapiro_pass'] and result['ks_pass'] and result['anderson_pass'] else '✗'\n",
    "    ax.set_title(f'Parameter {param_idx} {status}\\n(p={result[\"shapiro_p\"]:.3f})', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Marginal Normality Analysis: {n_all_pass}/{n_params} parameters passed all tests', \n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.show()\n",
    "\n",
    "# Statistical test for p-value uniformity (should be uniform if samples are truly normal)\n",
    "# Kolmogorov-Smirnov test for uniformity\n",
    "uniform_ks_shapiro, uniform_p_shapiro = stats.kstest(shapiro_pvals, 'uniform')\n",
    "uniform_ks_ks, uniform_p_ks = stats.kstest(ks_pvals, 'uniform')\n",
    "\n",
    "print(f\"\\n=== P-VALUE UNIFORMITY TESTS ===\")\n",
    "print(f\"(p-values should be uniformly distributed if marginals are truly normal)\")\n",
    "print(f\"Shapiro p-values: K-S stat={uniform_ks_shapiro:.4f}, p={uniform_p_shapiro:.4f}\")\n",
    "print(f\"  Result: {'✓ PASS' if uniform_p_shapiro > 0.05 else '✗ FAIL'} (uniform distribution)\")\n",
    "print(f\"K-S p-values:     K-S stat={uniform_ks_ks:.4f}, p={uniform_p_ks:.4f}\")\n",
    "print(f\"  Result: {'✓ PASS' if uniform_p_ks > 0.05 else '✗ FAIL'} (uniform distribution)\")\n",
    "\n",
    "# Overall assessment\n",
    "print(f\"\\n=== FINAL ASSESSMENT ===\")\n",
    "frac_primary = n_shapiro_pass / n_params\n",
    "\n",
    "if frac_primary >= 0.99:\n",
    "    print(\"✅ EXCELLENT: ≳99% of marginals pass Shapiro–Wilk.\")\n",
    "elif frac_primary >= 0.95:\n",
    "    print(\"✅ GOOD: ≥95% of marginals pass Shapiro–Wilk.\")\n",
    "elif frac_primary >= 0.90:\n",
    "    print(\"⚠️ BORDERLINE: 90–95% pass; inspect failed parameters.\")\n",
    "else:\n",
    "    print(\"❌ ISSUES: <90% pass; check sampling/model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc01324c",
   "metadata": {},
   "source": [
    "There are more than 90\\% tests that passed.\n",
    "\n",
    "We have to keep in mind that even if all parameters are perfectly normal, we expect 5\\% of rejections (5% of 168 is about 8 fails).\n",
    "\n",
    "Our sample size is large (> 300), Shapiro-Wilk in sensitive to tiny deviations. Therefore best to look at QQ-plots;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f284037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Take first N failed params for detailed inspection\n",
    "N = min(12, len(failed_params))\n",
    "n_cols = 4\n",
    "n_rows = math.ceil(N / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for j in range(N):\n",
    "    idx = failed_params[j]\n",
    "    ax = axes[j]\n",
    "    param_samples = samples[:, idx]\n",
    "    stats.probplot(param_samples, dist=\"norm\", plot=ax)\n",
    "    \n",
    "    r = normality_results[idx]\n",
    "    status = '✗'\n",
    "    ax.set_title(\n",
    "        f'Param {idx} {status}\\n'\n",
    "        f'SW p={r[\"shapiro_p\"]:.3g}, KS p={r[\"ks_p\"]:.3g}',\n",
    "        fontsize=9\n",
    "    )\n",
    "\n",
    "# Hide unused axes if any\n",
    "for k in range(N, len(axes)):\n",
    "    fig.delaxes(axes[k])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d524b",
   "metadata": {},
   "source": [
    "Let us check QQ-correlation and plot histogram + KDE + fitted normal pdf overlays for them. \n",
    "\n",
    "0.995–0.999 => small mild deviation (almost imperceptible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a10f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def qq_corr(sample):\n",
    "    x = np.sort(sample)\n",
    "    n = len(x)\n",
    "    p = (np.arange(1, n+1) - 0.5) / n\n",
    "    th = stats.norm.ppf(p)\n",
    "    return np.corrcoef(th, x)[0, 1]\n",
    "\n",
    "# Compute QQ correlation for each parameter\n",
    "qq_corrs = np.array([qq_corr(samples[:, i]) for i in range(samples.shape[1])])\n",
    "\n",
    "# Indices of \"worst\" parameters = lowest QQ correlation\n",
    "N_WORST = 6  # change to 10, 12, ... if you want more\n",
    "worst_idx = np.argsort(qq_corrs)[:N_WORST]\n",
    "\n",
    "print(\"Worst parameters by QQ-correlation:\")\n",
    "for i in worst_idx:\n",
    "    print(f\"Param {i}: qq_corr = {qq_corrs[i]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde, norm\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(N_WORST / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "for ax, idx in zip(axes, worst_idx):\n",
    "    data = samples[:, idx]\n",
    "    mu = np.mean(data)\n",
    "    sigma = np.std(data, ddof=1)\n",
    "\n",
    "    # Histogram (empirical density)\n",
    "    ax.hist(data, bins='auto', density=True, alpha=0.4, edgecolor='black', label='Histogram')\n",
    "\n",
    "    # KDE (smooth empirical density)\n",
    "    kde = gaussian_kde(data)\n",
    "    x_min, x_max = np.min(data), np.max(data)\n",
    "    x_grid = np.linspace(x_min, x_max, 400)\n",
    "    ax.plot(x_grid, kde(x_grid), linewidth=2, label='KDE')\n",
    "\n",
    "    # Fitted normal density\n",
    "    ax.plot(x_grid, norm.pdf(x_grid, loc=mu, scale=sigma),\n",
    "            linestyle='--', linewidth=2, label=f'Normal N({mu:.3g},{sigma:.3g}²)')\n",
    "\n",
    "    # Title with stats\n",
    "    # If you still have normality_results:\n",
    "    r = normality_results[idx]\n",
    "    ax.set_title(\n",
    "        f\"Param {idx}\\n\"\n",
    "        f\"qq={qq_corrs[idx]:.4f}, SW p={r['shapiro_p']:.3g}\",\n",
    "        fontsize=10\n",
    "    )\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for k in range(len(worst_idx), len(axes)):\n",
    "    fig.delaxes(axes[k])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb738932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
